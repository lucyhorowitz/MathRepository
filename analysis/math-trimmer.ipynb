{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e33ef05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f847f5",
   "metadata": {},
   "source": [
    "OKAY, here is a copy of trimmer that I will try to change to get good at just separating out math. \n",
    "\n",
    "Inspired by this link: https://stackoverflow.com/questions/51728211/math-expressions-in-spacy\n",
    "\n",
    "Still going to get rid of link-aliasing and metadata, but leaving in dollar signs.\n",
    "\n",
    "Going to replace all dollar signs with single dollar signs. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0acf85ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this should be done before cleaning links\n",
    "def remove_tags(text):\n",
    "    text = text.replace('#todo', '')\n",
    "    text = text.replace('#write_proof', '')\n",
    "    words = [word for word in text.split() if not word.startswith('https')]\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f87c22f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this should be done after removing tags.\n",
    "def remove_aliases(text=''):\n",
    "    start_indices = [match.start() for match in re.finditer('---', text)]\n",
    "    i = 0\n",
    "    while i < len(start_indices):\n",
    "        next_line = text.find('---', start_indices[i] + 4)\n",
    "        text = text.replace(text[start_indices[i]:next_line +3], (next_line +3 - start_indices[i])* '#')\n",
    "        i = i + 2\n",
    "    text = text.replace('#', '')\n",
    "    return text\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc72ddd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleans up in-text aliases and also removes octothorpes and residual brackets\n",
    "# should be done after removing tags\n",
    "def clean_links(text):\n",
    "    start_indices = [match.start() for match in re.finditer('\\[\\[', text)]\n",
    "    for index in start_indices:\n",
    "        next_pipe = text.find('|', index)\n",
    "        close_index = text.find(']', index)\n",
    "        if next_pipe != -1:\n",
    "            if next_pipe - close_index < 0:\n",
    "                text = text.replace(text[index:next_pipe + 1], (next_pipe - index + 1)*'#')\n",
    "    text = text.replace('#', '')\n",
    "    text = text.replace('[[', '')\n",
    "    text = text.replace(']]', '')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "90e5fc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_simples(text):\n",
    "    text = text.replace('.$$', ' $$.')\n",
    "    text = text.replace('$$', '$')\n",
    "    text = text.replace('$', ' $ ')\n",
    "    \n",
    "    text = text.replace('**', ' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92bb43fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do this before prepping spaces and removing tex\n",
    "def remove_environments(text):\n",
    "    start_indices = [match.start() for match in re.finditer('begin', text)]\n",
    "    i = 0\n",
    "    while i < len(start_indices):\n",
    "        end_index = text.find('\\end', start_indices[i] + 5)\n",
    "        end_end = text.find('}', end_index)\n",
    "        text = text.replace(text[start_indices[i]:end_end], (end_end - start_indices[i])* '#')\n",
    "        i = i + 1\n",
    "    text = text.replace('#', '')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d6e61fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_w_spaces(text):\n",
    "    text = text.replace('\\\\\\\\', ' \\\\')\n",
    "    text = text.replace('(', '(')\n",
    "    text = text.replace(')', ')')\n",
    "    text = text.replace('\\{', '{')\n",
    "    text = text.replace('\\}', '}') \n",
    "    text = text.replace(',', ', ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfa5de20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do this after removing all #s, i.e. last\n",
    "def remove_unreadable_tex(text):\n",
    "    #fonts\n",
    "    text = text.replace('\\\\mathbb', '')\n",
    "    text = text.replace('\\\\text', '')\n",
    "    text = text.replace('\\\\mathcal', '')\n",
    "    text = text.replace('\\\\mathfrak', '')\n",
    "    text = text.replace('\\\\mathbf', '')\n",
    "\n",
    "    #greek letters\n",
    "    text = text.replace('\\\\alpha', 'α')\n",
    "    text = text.replace('\\\\beta', 'β')\n",
    "    text = text.replace('\\\\gamma', 'γ')\n",
    "    text = text.replace('\\\\Gamma', 'Γ')\n",
    "    text = text.replace('\\\\delta', 'δ')\n",
    "    text = text.replace('\\\\Delta', 'Δ')\n",
    "    text = text.replace('\\\\epsilon', 'ε')\n",
    "    text = text.replace('\\\\varepsilon', 'ε')\n",
    "    text = text.replace('\\\\zeta', 'ζ')\n",
    "    text = text.replace('\\\\eta', 'η')\n",
    "    text = text.replace('\\\\theta', 'θ')\n",
    "    text = text.replace('\\\\Theta', 'Θ')\n",
    "    text = text.replace('\\\\iota', 'ι')\n",
    "    text = text.replace('\\\\kappa', 'κ')\n",
    "    text = text.replace('\\\\lambda', 'λ')\n",
    "    text = text.replace('\\\\Lambda', 'Λ')\n",
    "    text = text.replace('\\\\mu', 'μ')\n",
    "    text = text.replace('\\\\nu', 'ν')\n",
    "    text = text.replace('\\\\chi', 'χ')\n",
    "    text = text.replace('\\\\omicron', 'ο')\n",
    "    text = text.replace('\\\\pi', 'π')\n",
    "    text = text.replace('\\\\Pi', 'Π')\n",
    "    text = text.replace('\\\\rho', 'ρ')\n",
    "    text = text.replace('\\\\sigma', 'σ')\n",
    "    text = text.replace('\\\\Sigma', 'Σ')\n",
    "    text = text.replace('\\\\tau', 'τ')\n",
    "    text = text.replace('\\\\upsilon', 'υ')\n",
    "    text = text.replace('\\\\psi', 'ψ')\n",
    "    text = text.replace('\\\\Psi', 'Ψ')\n",
    "    text = text.replace('\\\\phi', 'φ')\n",
    "    text = text.replace('\\\\varphi', 'φ')\n",
    "    text = text.replace('\\\\Phi', 'Φ')\n",
    "    text = text.replace('\\\\xi', 'ξ')\n",
    "    text = text.replace('\\\\omega', 'ω')\n",
    "    text = text.replace('\\\\Omega', 'Ω')\n",
    "\n",
    "    #other replacements\n",
    "    text = text.replace('\\\\dots', '...')\n",
    "    text = text.replace('\\\\cdots', '···')\n",
    "    text = text.replace('\\\\leq', ' less than or equal to ')\n",
    "    text = text.replace('\\\\geq', ' greater than or equal to ')\n",
    "    text = text.replace('\\\\mid', ' such that ')\n",
    "    text = text.replace('\\\\partial', '∂')\n",
    "    text = text.replace('\\\\circ', ' composed with ')\n",
    "    text = text.replace('\\\\cap', ' intersected with ')\n",
    "    text = text.replace('\\\\setminus', '\\\\')\n",
    "    text = text.replace('\\\\sim', '~')\n",
    "    text = text.replace('\\\\int', '∫')\n",
    "    text = text.replace('\\\\infty', '∞')\n",
    "    text = text.replace('\\\\ell', 'l')\n",
    "    text = text.replace('\\\\mapsto', ' maps to')\n",
    "    text = text.replace('\\\\cup', ' union with ')\n",
    "    text = text.replace('\\\\bigcup', ' union of ')\n",
    "    text = text.replace('\\\\bigcaup', ' intersection of ')\n",
    "    text = text.replace('\\\\neq', '≠')\n",
    "    text = text.replace('\\\\cdot', '·')\n",
    "    text = text.replace('\\\\sqrt', '√')\n",
    "    text = text.replace('\\\\sharp', '#')\n",
    "    text = text.replace('\\\\supseteq', ' a superset of ')\n",
    "    text = text.replace('\\\\supset', ' a superset of ')\n",
    "    text = text.replace('\\\\subseteq', ' a subset of ')\n",
    "    text = text.replace('\\\\subset', ' a subset of ')\n",
    "    text = text.replace('\\\\cong', ' = ')\n",
    "    text = text.replace('\\\\simeq', ' = ' )\n",
    "    text = text.replace('=', ' = ' )\n",
    "    text = text.replace('\\\\langle', '«')\n",
    "    text = text.replace('\\\\rangle', '»')\n",
    "    text = text.replace('\\\\oplus', ' + ')\n",
    "    text = text.replace('\\\\times', ' times ')\n",
    "    text = text.replace('\\\\prod', ' product over ')\n",
    "    text = text.replace('\\\\equiv', ' ≈ ')\n",
    "    text = text.replace('\\\\emptyset', ' ø ')\n",
    "    text = text.replace('\\\\exists', ' there exists ')\n",
    "    text = text.replace('\\\\forall', ' for all ')\n",
    "    text = text.replace('\\\\quad', ' \\t ')\n",
    "    text = text.replace('&=', '= ')\n",
    "    text = text.replace('\\\\in', ' in ')\n",
    "    text = text.replace('\\\\to', ' to ')\n",
    "    text = text.replace(':', ': ')\n",
    "    \n",
    "    \n",
    "    #removals\n",
    "    text = text.replace('\\\\left', '')\n",
    "    text = text.replace('\\\\right', '')\n",
    "    text = text.replace('\\\\overline', '')\n",
    "    text = text.replace('\\\\underline', '')\n",
    "\n",
    "    #fractions\n",
    "    text = text.replace('\\\\frac', '')\n",
    "    text = text.replace('\\}\\{', '/')\n",
    "\n",
    "    return text\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21455254",
   "metadata": {},
   "outputs": [],
   "source": [
    "def better_tex_remove(text):\n",
    "    #fonts\n",
    "    text = text.replace('\\\\mathbb', '')\n",
    "    text = text.replace('\\\\text', '')\n",
    "    text = text.replace('\\\\mathcal', '')\n",
    "    text = text.replace('\\\\mathfrak', '')\n",
    "    text = text.replace('\\\\mathbf', '')\n",
    "\n",
    "    #greek letters\n",
    "    text = text.replace('\\\\alpha', 'α')\n",
    "    text = text.replace('\\\\beta', 'β')\n",
    "    text = text.replace('\\\\gamma', 'γ')\n",
    "    text = text.replace('\\\\Gamma', 'Γ')\n",
    "    text = text.replace('\\\\delta', 'δ')\n",
    "    text = text.replace('\\\\Delta', 'Δ')\n",
    "    text = text.replace('\\\\epsilon', 'ε')\n",
    "    text = text.replace('\\\\varepsilon', 'ε')\n",
    "    text = text.replace('\\\\zeta', 'ζ')\n",
    "    text = text.replace('\\\\eta', 'η')\n",
    "    text = text.replace('\\\\theta', 'θ')\n",
    "    text = text.replace('\\\\Theta', 'Θ')\n",
    "    text = text.replace('\\\\iota', 'ι')\n",
    "    text = text.replace('\\\\kappa', 'κ')\n",
    "    text = text.replace('\\\\lambda', 'λ')\n",
    "    text = text.replace('\\\\Lambda', 'Λ')\n",
    "    text = text.replace('\\\\mu', 'μ')\n",
    "    text = text.replace('\\\\nu', 'ν')\n",
    "    text = text.replace('\\\\chi', 'χ')\n",
    "    text = text.replace('\\\\omicron', 'ο')\n",
    "    text = text.replace('\\\\pi', 'π')\n",
    "    text = text.replace('\\\\Pi', 'Π')\n",
    "    text = text.replace('\\\\rho', 'ρ')\n",
    "    text = text.replace('\\\\sigma', 'σ')\n",
    "    text = text.replace('\\\\Sigma', 'Σ')\n",
    "    text = text.replace('\\\\tau', 'τ')\n",
    "    text = text.replace('\\\\upsilon', 'υ')\n",
    "    text = text.replace('\\\\psi', 'ψ')\n",
    "    text = text.replace('\\\\Psi', 'Ψ')\n",
    "    text = text.replace('\\\\phi', 'φ')\n",
    "    text = text.replace('\\\\varphi', 'φ')\n",
    "    text = text.replace('\\\\Phi', 'Φ')\n",
    "    text = text.replace('\\\\xi', 'ξ')\n",
    "    text = text.replace('\\\\omega', 'ω')\n",
    "    text = text.replace('\\\\Omega', 'Ω')\n",
    "\n",
    "    text = text.replace('\\\\ell', 'l')\n",
    "    text = text.replace('\\\\exists', ' there exists ')\n",
    "    text = text.replace('\\\\forall', ' for all ')\n",
    "    text = text.replace('\\\\quad', ' \\t ')\n",
    "    text = text.replace('&=', '= ')\n",
    "    text = text.replace('\\\\in', ' in ')\n",
    "    text = text.replace('\\\\to', ' to ')\n",
    "    text = text.replace(':', ': ')\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7544a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_sweep(text):\n",
    "    words = [word.replace('\\\\', '') for word in text.split() if not word.startswith('\\\\') and not word.startswith('Helvetica')]\n",
    "    text = ' '.join(words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "adf30156",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_everything(text):\n",
    "    text = remove_tags(text) #done first\n",
    "    text = clean_links(text)\n",
    "    text = remove_aliases(text)\n",
    "    text = remove_environments(text)\n",
    "\n",
    "    #text = remove_unreadable_tex(text)\n",
    "    text = better_tex_remove(text)\n",
    "\n",
    "    text = remove_simples(text)\n",
    "    text = prep_w_spaces(text)\n",
    "\n",
    "    text = final_sweep(text)\n",
    "    return text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1aef711b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# running this thing on every file in definitions and then adding the conll \n",
    "# to a text file one at a time...\n",
    "\n",
    "import os\n",
    "import spacy\n",
    "\n",
    "#from spacy_conll import init_parser\n",
    "#nlp = init_parser(\"en_core_web_lg\", 'spacy')\n",
    "\n",
    "from spacy_conll import init_parser\n",
    "nlp = init_parser(\"en_core_web_lg\", 'spacy')\n",
    "\n",
    "from spacy.tokens import Token\n",
    "Token.set_extension('is_TeX', default=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76efe7ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\tThe\tthe\tDET\tDT\tDefinite=Def|PronType=Art\t3\tdet\t_\t_\n",
      "2\tadjoint\tadjoint\tADJ\tJJ\tDegree=Pos\t3\tamod\t_\t_\n",
      "3\tmap\tmap\tNOUN\tNN\tNumber=Sing\t9\tnsubj\t_\t_\n",
      "4\tof\tof\tADP\tIN\t_\t3\tprep\t_\t_\n",
      "5\ta\ta\tDET\tDT\tDefinite=Ind|PronType=Art\t7\tdet\t_\t_\n",
      "6\tLie\tlie\tNOUN\tNN\tNumber=Sing\t7\tcompound\t_\t_\n",
      "7\talgebra\talgebra\tNOUN\tNN\tNumber=Sing\t4\tpobj\t_\t_\n",
      "8\t$ g $\t$ g $\tSYM\t$\t_\t3\tappos\t_\t_\n",
      "9\tis\tbe\tAUX\tVBZ\tMood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin\t0\tROOT\t_\t_\n",
      "10\tthe\tthe\tDET\tDT\tDefinite=Def|PronType=Art\t11\tdet\t_\t_\n",
      "11\tmap\tmap\tNOUN\tNN\tNumber=Sing\t9\tattr\t_\t_\n",
      "12\t$ ad: g \\to End(g) $\t$ ad: g \\to End(g) $\tNOUN\tNN\tNumber=Sing\t9\tattr\t_\t_\n",
      "13\tthat\tthat\tPRON\tWDT\tPronType=Rel\t14\tnsubj\t_\t_\n",
      "14\tsends\tsend\tVERB\tVBZ\tNumber=Sing|Person=3|Tense=Pres|VerbForm=Fin\t12\trelcl\t_\t_\n",
      "15\t$ X $\t$ x $\tNOUN\tNN\tNumber=Sing\t14\tdobj\t_\t_\n",
      "16\tto\tto\tPART\tTO\t_\t17\tprep\t_\t_\n",
      "17\t$ ad_X $\t$ ad_x $\tNOUN\tNN\tNumber=Sing\t14\txcomp\t_\t_\n",
      "18\tin\tin\tADP\tIN\t_\t14\tprep\t_\t_\n",
      "19\tthe\tthe\tDET\tDT\tDefinite=Def|PronType=Art\t20\tdet\t_\t_\n",
      "20\tset\tset\tNOUN\tNN\tNumber=Sing\t18\tpobj\t_\t_\n",
      "21\tof\tof\tADP\tIN\t_\t20\tprep\t_\t_\n",
      "22\tendomorphisms\tendomorphism\tNOUN\tNNS\tNumber=Plur\t21\tpobj\t_\t_\n",
      "23\tof\tof\tADP\tIN\t_\t22\tprep\t_\t_\n",
      "24\t$ g $\t$ g $\tNUM\tCD\tNumType=Card\t23\tpobj\t_\tSpaceAfter=No\n",
      "25\t.\t.\tPUNCT\t.\tPunctType=Peri\t9\tpunct\t_\tSpaceAfter=No\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from spacy.language import Language\n",
    "@Language.component('detextor')\n",
    "def detextor(doc):\n",
    "    dollar_indices = [index for index, token in enumerate(doc) if token.text == '$']\n",
    "    while len(dollar_indices) > 1:\n",
    "        with doc.retokenize() as retokenizer:\n",
    "            attrs = {'is_TeX': True}\n",
    "            retokenizer.merge(doc[dollar_indices[0]:dollar_indices[1] + 1], attrs=attrs)\n",
    "        dollar_indices = [index for index, token in enumerate(doc) if token.text == '$']\n",
    "    return doc\n",
    "# nlp.remove_pipe('detextor') #might need to add this back in if you run this block more than once\n",
    "nlp.add_pipe('detextor', after='ner')\n",
    "\n",
    "doc = nlp('The adjoint map of a Lie algebra $ g $ is the map $ ad: g \\\\to End(g) $ that sends $ X $ to $ ad_X $ in the set of endomorphisms of $ g $.') \n",
    "\n",
    "print(doc._.conll_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c6b22611",
   "metadata": {},
   "outputs": [],
   "source": [
    "definitions = '/Users/lucyhorowitz/Library/Mobile Documents/iCloud~md~obsidian/Documents/MathRepository/notes/definitions'\n",
    "i = 1\n",
    "sent_lengths = []\n",
    "for filepath in os.listdir(definitions):\n",
    "    defn = os.path.join(definitions, filepath)\n",
    "    defn_text = open(defn).read()\n",
    "    file_name = os.path.basename(filepath)\n",
    "\n",
    "    with open('defs-with-separated-tex.conllu', 'a') as f:\n",
    "        defn_text = clean_everything(defn_text)\n",
    "        doc = nlp(defn_text)\n",
    "        sents = [sent for sent in doc.sents]\n",
    "        f.write('# defn_id = ' + str(i) + '\\n')\n",
    "        f.write('# defn_title = ' + file_name + '\\n')\n",
    "        j = 1\n",
    "        for sent in doc.sents:\n",
    "            doc2 = nlp(sent.text)\n",
    "            conll = doc2._.conll_str\n",
    "            sent_lengths.append(len(doc2))\n",
    "            f.write('# sent_id = ' + str(j) + '\\n')\n",
    "            f.write('# sent_len = ' + str(len(doc2)) + '\\n')\n",
    "            f.write('# text = ' + sent.text + '\\n')\n",
    "            f.write(conll + '\\n')\n",
    "            j = j + 1\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0b8295",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "\n",
    "print('Median sentence length: ' + str(statistics.median(sent_lengths)))\n",
    "print('Mean sentence length: ' + str(statistics.mean(sent_lengths)))\n",
    "print('Standard deviation of sentence length: ' + str(statistics.stdev(sent_lengths)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455c78d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# An \"interface\" to matplotlib.axes.Axes.hist() method\n",
    "n, bins, patches = plt.hist(x=sent_lengths, bins=np.arange(min(sent_lengths), max(sent_lengths) + 1, 1), color='#0504aa',\n",
    "                            alpha=0.7, rwidth=3)\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.xlabel('# of words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of defn sentence lengths')\n",
    "maxfreq = n.max()\n",
    "# Set a clean upper y-axis limit.\n",
    "plt.ylim(ymax=np.ceil(maxfreq / 10) * 10 if maxfreq % 10 else maxfreq + 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866c0c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from operator import itemgetter\n",
    "text = clean_everything(open(\"Definitions.rtf\").read())\n",
    "words = [word for word in text.split() if word.lower() not in nlp.Defaults.stop_words]\n",
    "\n",
    "mega_doc = nlp(' '.join(words))\n",
    "\n",
    "counting_dict = mega_doc.count_by(spacy.attrs.IDS['LEMMA'])\n",
    "\n",
    "ordered = OrderedDict(sorted(counting_dict.items(), key = itemgetter(1), reverse=True))\n",
    "\n",
    "for lemma, count in ordered.items():\n",
    "     human_readable_tag = mega_doc.vocab[lemma].text\n",
    "     print(human_readable_tag, count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
